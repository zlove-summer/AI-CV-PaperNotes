## 0 论文基本信息

**论文**：ImageNet Classification with Deep Convolutional Neural Networks

**时间**：2012年

**期刊**：NIPS

code：https://github.com/zlove-summer/AlexNet-pytorch

## 1 摘要

文章训练了一个深度神经网络模型用于ImageNet图像分类竞赛，取得了大赛的冠军并且效果大大好于第二名。模型包括了5个卷积层和3个全连接层，使用了GPU进行加快训练。文章主要提出了Relu激活函数、DropOut正则化这些创新。

## 2 创新点

- 使用Relu激活函数，取代了Sigmoid和tanh。
- 提出DropOut正则化，以防止过拟合。
- 采用了数据增强手段进行过拟合。

## 3 背景介绍

当时的分类问题，主要是**手工提取特征**，然后放入如SVM等分类器进行分类。神经网络参数较大，当时的GPU性能不够优秀，所以没有被太多人重视。作者**训练出了深度卷积神经网络用于图像分类任务**，为一大首创，并且掀起了深度学习一大热潮。

## 4 方法

### 4.1 模型架构

以下图是AlexNet模型的总体设计。

![image-20211217202457023](https://user-images.githubusercontent.com/81303574/175772536-9a3d6288-f524-4ec3-a5e0-7f45012eae30.png)

- 模型的输入是224\*224\*3（图中有误），输出是经过softmax后的1000维的向量，代表1000个类别。
- 模型包括5个卷积层和3个全连接层，采用重叠池化等操作。其他卷积核大小和步长图中均有体现。

### 4.2 主要创新点

#### 4.2.1 Relu激活函数

​		以前的神经网络模型，一般采用tanh或者sigmiod激活函数。文中采用Relu激活函数，并且后来relu激活函数也作为了深度学习主要的激活函数。Relu激活函数的形式是：$f(x)=max(0,x)$。这种方式，使得更大的神经刺激能更好地向后层传递，在深层网络中，**能缓解梯度消失的情况，并且由于梯度求解非常容易等优点，被广泛使用**。这种方式也使得训练速度大大加快，模型更快收敛。


![image-20220417211021908](https://user-images.githubusercontent.com/81303574/175772554-6f0eee26-c72e-4256-a33a-f961b6a3932c.png)


#### 4.2.2 Dropout防止过拟合

深度神经网络网络参数很多，模型很大，防止过拟合非常重要。文章采用了Dropout防止过拟合。

> Dropout：每次epoch训练时，随机将隐藏层的输出置0，被dropout的神经元将不会参加前馈和反馈过程。

Dropout也可以称为一种正则化手段。

#### 4.2.3 数据增强

采用了随机裁剪、PCA改变图片亮度等手段进行数据增强，防止过拟合。

### 4.3 模型表现

以下是ImageNet图像数据集竞赛上，ALexNet模型的表现。从图中可以看出top1和top5错误率均大大降低，效果非常好（其他两种模型均是传统手工提取特征的模型）。

![image-20211217204907388](https://user-images.githubusercontent.com/81303574/175772764-a985500b-3cd3-4557-9486-3a0d80e1bfd4.png)

## 5 结论

​		AlexNet开启了深度学习的热潮，其中很多手段比如Relu和Dropout直至今日依然在用，这种开创一个新的模块的勇气和创新值得学习。

